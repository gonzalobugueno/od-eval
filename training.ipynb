{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install --quiet keras-cv tensorflow\n",
    "!pip install --quiet keras pillow pandas matplotlib opencv-python"
   ],
   "id": "5be9483d09e0c905"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T00:10:43.863225Z",
     "start_time": "2025-08-06T00:10:04.343110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import *\n",
    "from debugging import *\n",
    "from analytics import *\n",
    "from augmentations import *\n",
    "from models import *\n",
    "import keras_cv\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "init()"
   ],
   "id": "99765d8d2dcf9722",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 01:10:05.098712: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-06 01:10:05.139180: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754439005.165860  397305 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754439005.175057  397305 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754439005.198980  397305 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754439005.198992  397305 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754439005.198994  397305 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754439005.198995  397305 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-06 01:10:05.207424: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/mnt/c/Users/n3/Desktop/od-eval/.linuxvnev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory growth enabled for GPU(s)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Datasets",
   "id": "ce56a945c0322b39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T00:34:44.086531Z",
     "start_time": "2025-08-06T00:34:44.084089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_ds = composite_ds(\n",
    "    multi_coin_ds([('./datasets/synthetic/annotations.xml', None)], togray=True, d_img=False, d_bb=False,\n",
    "                  tsz=(1024, 1024)),\n",
    "    multi_coin_ds([('./datasets/synthetic/annotations.xml', None)], togray=True, d_img=False, d_bb=False,\n",
    "                  tsz=(1024, 1024)).map(apply_directional_blur(45, 11)),\n",
    "    multi_coin_ds([('./datasets/synthetic/annotations.xml', None)], togray=True, d_img=False, d_bb=False,\n",
    "                  tsz=(1024, 1024)).map(apply_gaussian_blur(kernel_size=15))\n",
    ").batch(4).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = multi_coin_ds([\n",
    "    ('./datasets/90/annotations.xml', 'PNG'),\n",
    "    ('./datasets/60/annotations.xml', 'PNG'),\n",
    "    ('./datasets/30/annotations.xml', 'PNG'),\n",
    "    ('./datasets/090/annotations.xml', 'PNG'),\n",
    "    ('./datasets/190/annotations.xml', 'PNG'),\n",
    "    ('./datasets/clear1/annotations.xml', None),\n",
    "    ('./datasets/clear2/annotations.xml', None),\n",
    "], visibility=['clear'], format='xywh', d_img=False, d_bb=False, tsz=(1024, 1024)).batch(2).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "s_ar = scale_ar_factor()"
   ],
   "id": "1efaa11f2f7b9471",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RetinaNet + EfficientNet Backbone",
   "id": "781e946d9dcb4fc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "backbone = keras_cv.models.EfficientNetV2Backbone.from_preset(\"efficientnetv2_b0_imagenet\", include_rescaling=True)\n",
    "backbone.trainable = False\n",
    "\n",
    "model = keras_cv.models.RetinaNet(\n",
    "    num_classes=1,\n",
    "    backbone=backbone,\n",
    "    anchor_generator=keras_cv.layers.AnchorGenerator(\n",
    "        bounding_box_format=\"xywh\",\n",
    "        sizes=[  # anchor size\n",
    "            4.0,\n",
    "            8.0,\n",
    "            10.0,\n",
    "            16.0,\n",
    "            32.0\n",
    "        ],\n",
    "        strides=[2 ** i for i in range(3, 8)],\n",
    "        aspect_ratios=[  # coin aspect ratios (w/h)\n",
    "            0.75 * s_ar,  # low distance\n",
    "            1.25 * s_ar,  # medium distance\n",
    "            1.80 * s_ar  # distant\n",
    "        ],\n",
    "        scales=[\n",
    "            1,\n",
    "            2 ** (1 / 3),\n",
    "            2 ** (2 / 3)\n",
    "        ],\n",
    "        clip_boxes=True,\n",
    "    ),\n",
    "    bounding_box_format=\"xywh\",\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(\n",
    "        learning_rate=0.0001,\n",
    "        weight_decay=0.0001,\n",
    "        amsgrad=False\n",
    "    ),\n",
    "    classification_loss=keras_cv.losses.FocalLoss(\n",
    "        from_logits=True,\n",
    "        alpha=0.25,\n",
    "        gamma=2.0,\n",
    "    ),\n",
    "    box_loss=keras_cv.losses.SmoothL1Loss(l1_cutoff=1.0)\n",
    ")\n",
    "\n",
    "history_head = model.fit(train_ds, epochs=5, validation_data=val_ds)"
   ],
   "id": "b5e50f059adb2c29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_history(history_head)",
   "id": "145c2a73afea8c36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# YOLOv8",
   "id": "10344f4419d37286"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
    "    \"yolo_v8_s_backbone_coco\",  # or \"yolo_v8_s_pretrained\", \"yolo_v8_l_pretrained\" etc.\n",
    "    include_rescaling=True\n",
    ")\n",
    "backbone.trainable = False  # Freeze backbone for fine-tuning\n",
    "\n",
    "model = keras_cv.models.YOLOV8Detector(\n",
    "    num_classes=1,\n",
    "    bounding_box_format=\"xywh\",\n",
    "    backbone=backbone,\n",
    "    fpn_depth=2  # You can adjust this based on your needs\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(\n",
    "        learning_rate=0.0001,\n",
    "        weight_decay=0.0001,\n",
    "        amsgrad=False\n",
    "    ),\n",
    "    classification_loss=keras_cv.losses.FocalLoss(\n",
    "        from_logits=True,\n",
    "        alpha=0.25,\n",
    "        gamma=2.0,\n",
    "    ),\n",
    "    box_loss=keras_cv.losses.SmoothL1Loss(l1_cutoff=1.0)\n",
    ")\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=5)"
   ],
   "id": "505d6ba3f735a6b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "backbone.trainable = True\n",
    "for layer in backbone.layers[:-10]:  # Keep first layers frozen\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(\n",
    "        learning_rate=0.0001,\n",
    "        weight_decay=0.0001,\n",
    "        amsgrad=False\n",
    "    ),\n",
    "    classification_loss=keras_cv.losses.FocalLoss(\n",
    "        from_logits=True,\n",
    "        alpha=0.25,\n",
    "        gamma=2.0,\n",
    "    ),\n",
    "    box_loss=keras_cv.losses.SmoothL1Loss(l1_cutoff=1.0)\n",
    ")\n",
    "\n",
    "model.fit(ds, validation_data=val, epochs=5)"
   ],
   "id": "14031b9f4464a923"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "it = iter(get_real_samples(\n",
    "    multi_coin_ds([('./datasets/190/annotations.xml', 'PNG')], d_img=False, d_bb=False, tsz=(1024, 1024))))"
   ],
   "id": "f21231e849f64c7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "img, boxes, classes = next(it)\n",
    "\n",
    "y = model.predict(np.expand_dims(img, axis=0))\n",
    "\n",
    "visualise_bundle(\n",
    "    (img.numpy() * 255).astype(np.uint8),\n",
    "    [y['boxes'][0][0]],\n",
    "    [y['classes'][0][0]]\n",
    ")\n",
    "print(\"confidence of \", np.max(y['confidence']))\n",
    "\n",
    "visualise_bundle(\n",
    "    (img.numpy() * 255).astype(np.uint8),\n",
    "    boxes,\n",
    "    classes\n",
    ")"
   ],
   "id": "8bd40146ca411d84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.75, verbose=1),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        'coin_best_3.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        mode='min',  # 'min' for val_loss, 'max' for val_accuracy\n",
    "        verbose=1\n",
    "    )\n",
    "],\n",
    "\n",
    "backbone.trainable = True  # Unfreeze for adaptation\n",
    "model.compile(optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-5))\n"
   ],
   "id": "9727af4eadb6d460"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "it = iter(get_real_samples(\n",
    "    multi_coin_ds([('./datasets/190/annotations.xml', 'PNG')], d_img=False, d_bb=False, tsz=(1024, 1024))))"
   ],
   "id": "d5cc0d63bccf3987"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "img, boxes, classes = next(it)\n",
    "\n",
    "y = model.predict(np.expand_dims(img, axis=0))\n",
    "\n",
    "idx = np.argmax(y['confidence'])\n",
    "\n",
    "visualise_bundle(\n",
    "    (img.numpy() * 255).astype(np.uint8),\n",
    "    y['boxes'][0],\n",
    "    y['classes'][0]\n",
    ")\n",
    "\n",
    "print(f\"confidence of {np.max(y['confidence'])} (i={idx})\")\n",
    "\n",
    "visualise_bundle(\n",
    "    (img.numpy() * 255).astype(np.uint8),\n",
    "    boxes,\n",
    "    classes\n",
    ")"
   ],
   "id": "36b6ee6db76c12dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "idx = np.argmax(y['confidence'])\n",
    "\n",
    "best_box = y['boxes'][idx:idx + 1]  # shape: (1, 4)\n",
    "best_score = y['confidence'][idx:idx + 1]  # shape: (1,)\n",
    "best_class = np.zeros_like(best_score, dtype=np.int32)\n",
    "\n",
    "# img_uint8 = (img.numpy() * 255).astype(np.uint8)\n",
    "#\n",
    "# keras_cv.visualization.plot_bounding_box_gallery(\n",
    "#     np.expand_dims(img, axis=0),\n",
    "#     value_range=(0, 255),\n",
    "#     rows=1,\n",
    "#     cols=1,\n",
    "#     y_true={\n",
    "#         \"boxes\": np.expand_dims(best_box, axis=0),\n",
    "#         \"classes\": np.expand_dims([0], axis=0)\n",
    "#     },\n",
    "#     scale=5,\n",
    "#     bounding_box_format=\"xywh\",\n",
    "#     class_mapping={int(k): f\"cls_{k}\" for k in best_class},\n",
    "# )\n",
    "#\n",
    "# plt.show()\n"
   ],
   "id": "e18f8849a52ec642"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "y['boxes'][0][idx:idx + 1]",
   "id": "8c2aa3675e16cbc0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
